import numpy as np
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist=input_data.read_data_sets("MNIST_data/",one_hot=True)

#outline the model

input_size=784
output_size=10
hidden_layer_size=80

#clears all varibles there in the previous runs
tf.reset_default_graph()


#Define the input and targets in the place holder
inputs=tf.placeholder(tf.float32,[None,input_size])
targets=tf.placeholder(tf.float32,[None,output_size])

#We calculate the weights and bias from the input layer to the output layer using Relu activation function

weights1=tf.get_variable("weights1",[input_size,hidden_layer_size])
biases1=tf.get_variable("biases1",[hidden_layer_size])

output1=tf.nn.relu(tf.matmul(inputs,weights1)+biases1)

weights2=tf.get_variable("weights2",[hidden_layer_size,hidden_layer_size])
biases2=tf.get_variable("biases2",[hidden_layer_size])

output2=tf.nn.relu(tf.matmul(output1,weights2)+biases2)

weights3=tf.get_variable("weights3",[hidden_layer_size,output_size])
biases3=tf.get_variable("biases3",[output_size])

output=tf.matmul(output2,weights3)+biases3

#We calculate the final output using softmax and optimize it using the ADAM optimizer

loss=tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=targets)
mean_loss=tf.reduce_mean(loss)
optimize=tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)

#We calculate the Accuracy of the model

out_equal_target=tf.equal(tf.argmax(output,1),tf.argmax(targets,1))
accuracy=tf.reduce_mean(tf.cast(out_equal_target,tf.float32))

#We initialize the session and run the global variables to activate them

sess=tf.InteractiveSession()
initializer=tf.global_variables_initializer()
sess.run(initializer)

batch_size=150
batches_number=mnist.train._num_examples // batch_size
print(mnist.train._num_examples)
print(batches_number)
max_epochs=15
prev_validation_loss=999999.9

for epoch_counter in range(max_epochs):
    
    #Training set
    curr_epoch_loss=0
    for batch_counter in range(batches_number):
        
        #we load the inputs and targets from the training set into the input and output batch
        input_batch,target_batch=mnist.train.next_batch(batch_size)
        
        _,batch_loss=sess.run([optimize,mean_loss],feed_dict={inputs:input_batch,targets:target_batch})
        
        curr_epoch_loss+=batch_loss
        
    curr_epoch_loss/=batches_number
    
    #Validation set and Foward propagation does not need a loop it is a single iteration
    input_batch,target_batch=mnist.validation.next_batch(mnist.validation.num_examples)
    
    validation_loss,validation_accuracy=sess.run([mean_loss,accuracy],feed_dict={inputs:input_batch,targets:target_batch})
    
    print("Epoch: " + str(epoch_counter+1)+ 
          ". Training_loss: " + "{0:.3f}".format(curr_epoch_loss) + 
          ". validation_loss: " + "{0:.3f}".format(validation_loss) + 
          ". validation_accuracy: " + "{0:.3f}".format(validation_accuracy))
    
    if validation_loss > prev_validation_loss:
        break
        
print("End of Training")

#Now we send batches to the test dataset (we only feed-foward propogate ) and calculate its accuracy.

input_batch,target_batch=mnist.test.next_batch(mnist.test.num_examples)
        
test_accuracy=sess.run([accuracy],feed_dict={inputs:input_batch,targets:target_batch})

test_accuracy_percent=test_accuracy[0]*100

print("Test Accuracy",test_accuracy_percent)
